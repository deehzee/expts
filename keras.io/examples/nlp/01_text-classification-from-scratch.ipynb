{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5af8a5a-8160-46a1-bb64-9f41bd4a4db8",
      "metadata": {
        "id": "d5af8a5a-8160-46a1-bb64-9f41bd4a4db8"
      },
      "source": [
        "# Text Classification from Scratch\n",
        "\n",
        "https://keras.io/examples/nlp/text_classification_from_scratch/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4054dbf6-8dd9-4813-ac5a-c10bcf95c3af",
      "metadata": {
        "id": "4054dbf6-8dd9-4813-ac5a-c10bcf95c3af"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f83dd71-d715-4dd8-bcc9-8c8a9eb6af2b",
      "metadata": {
        "id": "7f83dd71-d715-4dd8-bcc9-8c8a9eb6af2b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d269588f-23ea-4fb2-9cf0-4317e930eeca",
      "metadata": {
        "id": "d269588f-23ea-4fb2-9cf0-4317e930eeca"
      },
      "source": [
        "## Load the data: IMDB movie review sentiment classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4cf216e-317e-4860-8576-5a84065b6486",
      "metadata": {
        "id": "e4cf216e-317e-4860-8576-5a84065b6486"
      },
      "source": [
        "Run these commands to download the data into `./data` folder:\n",
        "```bash\n",
        "$ curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "$ tar -xf aclImdb_v1.tar.gz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e5c99c-6ec4-48df-badb-04bca3544da1",
      "metadata": {
        "id": "16e5c99c-6ec4-48df-badb-04bca3544da1",
        "outputId": "b0937c3f-f982-442b-aac2-2b4dbf60eedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=2405,\n",
        ")\n",
        "\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=2405,\n",
        ")\n",
        "\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/aclImdb/test',\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "print(f'Number of batches in raw_train_ds: {tf.data.experimental.cardinality(raw_train_ds)}')\n",
        "print(f'Number of batches in raw_val_ds: {tf.data.experimental.cardinality(raw_val_ds)}')\n",
        "print(f'Number of batches in raw_test_ds: {tf.data.experimental.cardinality(raw_test_ds)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2cfc2d-ba9e-4dad-8f96-c306d8961e55",
      "metadata": {
        "id": "bb2cfc2d-ba9e-4dad-8f96-c306d8961e55",
        "outputId": "d4f45b3d-226d-4785-aa1c-26e346047daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'We\\'re talking about a low budget film, and it\\'s understandable that there are some weaknesses (no spoilers: one sudden explosives expert and one meaningless alcoholic); but in general the story keeps you interested, most of the characters are likable and there are some original situations. <br /><br />I really like films that surprise you with some people that are not who they want you to believe and then twist and turn the plot ... I applaud this one on that. <br /><br />If you know what I mean, try to see also \"Nueve Reinas\" (Nine Queens) a film from Argentina.'\n",
            "1\n",
            "\n",
            "b\"I've seen this movie, when I was traveling in Brazil. I found it difficult to really understand Brazilian culture and society, because it has so many regional and class differences. To see this movie in Sao Paulo itself was a revelation. It shows something of the everyday life of many Brazilians. On the other side, it is sometimes a little bit over-dramatized. And that's the only negative comment I have on this film. It's sometimes too much, too much sex, too many murders and too much cynicism for one film. The director could film some things a bit more subtle, it would make the film more effective.<br /><br />Despite this I liked the movie and the way the story unravels itself. The characters are complex, and very much like real-life people. Not pretty American actors and actresses with a lot of cosmetics, but people who could be ugly and beautiful at the same time. That makes the film realistic, even when the story is not that convincing.\"\n",
            "1\n",
            "\n",
            "b\"Those French and those Germans sure have a long history of not liking each other. It is interesting to note that Kamerdaschaft or Comradeship in translation takes place in 1931. Only a few years later, Hitler would siege Germany and begin his plans to take over the world, France being a casualty of his ambitions. But these are times of sereneness compared to the future. A group of miners at the border try to cross over to France to get work. They are spurned back and later at a nightclub by their French neighbors. Then a disaster happens in the mines of the French and a well-crafted and written scene, a troupe of German miners decide to come to the rescue. A simple story is it not? Pabst was a poet of silent cinema and I am not sure if this is his first sound movie or not, but his poetry is there to be discovered. He isn't fussy but brings a rugged realism to the ordeal. Ther is even a flashback to a WWII event that beckons the point of this story. Supposedly based on a real event, the movie does the events proudly with directness and terseness. Smetimes, that's what a movie needs to be.\"\n",
            "1\n",
            "\n",
            "b'I had never read Gary Paulsen\\'s novel, Hatchet, for which \\'A Cry in the Wild\\' is the adaptation of, so I can\\'t make any comparisons to the book. I will, however, say that as a film on its own, adaptation or no adaptation, it was an underdeveloped adventure that provides no major explanation of its few characters.<br /><br />Think of \\'A Cry in the Wild\\' as a less luxurious, teenage mountaineer (was Quincy, California the only place this was filmed?) version of \\'Cast Away.\\' Jared Rushton is 13-year-old Brian Roebson, a kid headed on a small plane to visit his father, until the craft crashes over some deserted mountain terrain, leaving the kid stranded for quite a while and having to defend himself.<br /><br />There are basically three parts to the film. The obvious being the ten or fifteen minute introduction of the characters, namely Brian and his mom. <br /><br />The next third of the movie (which really consumes nearly all of the film) is that of Brian \"roughing it.\" These scenes contain no particularly amazing action, nothing spectacular other than lots of beautiful cinematography of a beautiful Yukon landscape. Nothing to put you on edge, no real encounters (except a brisk confrontation with a cub), and no major dilemmas to initiate some sort of enjoyment or connection with the character on the screen. You might even feel briefly bored with the passage of time as we witness Brian dealing with his situation through first, primitive means, and then more improved ones (using tools, etc) for his survival. It is more like the ordinary time that passes if you were actually stuck in the situation, and that is pretty much about it. In other words, they put no meat on the Paulsen\\'s words when they translated them into a visual media.<br /><br />And, of course, the third part of the movie is his rescue.<br /><br />There is a subplot that continuously seeks to make itself known during this time, however. Some conflict between Brian and his parents that created a rocky, awkward relationship between them. However, for the most part, it is only explained in brief, intermittent, minimal dialog flashbacks that look more like a back story for a music video. Any minute, the singer from Jefferson Starship, should chime in an start singing \\'Sara.\\' Other than what the viewer can draw from the implications, or guess for his own need to fill the gaps in the narrative, we get a very underdeveloped back story which was probably necessary to enjoy at least part of this film and create a connection to the characters, whether or not it really had anything to do with Brian\\'s survival adventure in the third part of the movie. These are the flaws in the narrative that through the viewer into a stupor as he struggles to find out what the heck those people there on the screen are doing and, for me, almost done to the point of screaming at the television to say something and tell me more! <br /><br />It certainly was not, for me, a good adventure tale. But, for fans of Jared Rushton, it was one of the last few movies he made. So, watch it purely for nostalgia, if nothing else.'\n",
            "0\n",
            "\n",
            "b'The best Laurel and Hardy shorts are filled to the brim with mishaps, accidents and destruction, mostly caused by Stan, but with Ollie receiving the bulk (!) of the punishment-- see the great \\'The Music Box\\' (1933) or \\'Towed in a Hole\\' (1932) as some some classic examples.<br /><br />Here, however, for some reason (is it because it was based on a sketch by Stan\\'s father?) the boys play it \\'straight\\' in a \\'comedy\\' built around jokes and supposedly funny situations. It doesn\\'t come off. It\\'s merely another third-rate tedious 30s comedy, heightened only by the personalities of Stan and Ollie who never really display any of their trademarked gestures (Ollie\\'s finger wiggling, Stan\\'s blank stares, etc.) or comic abilities.<br /><br />The film begins with them running from the police. Since we never see or know why, it\\'s hard to believe or accept their fear of being caught, and thus hiding in Colonel Buckshot\\'s mansion. The premise for the \\'humor\\', Ollie passing himself off as the Colonel and Stan passing himself off as both the butler and the maid are never very engaging. They are not playing \\'Stan and Ollie\\' in this film. Their parts could have been played by any of the pedestrian studio actors and it would be just as poor.<br /><br />Stan could mime and make whatever he would do funny, but he doesn\\'t get the chance to do any of that here. He\\'s constrained by uttering too much dialog to \\'move\\' the plot, but none of it rises much above the silly. We are treated to endless third rate comedy chestnuts such as the running gag of not correctly pronouncing Lord Plumtree\\'s name, the \"Call me a cab! Okay you\\'re a cab!\" joke, cops losing their clothes and being seen in long johns, and a non-sequiter ending of Stan and Ollie as the two parts in a painfully obvious horse costume as they make their escape on a bicycle for two, and James Finlayson is still doing his silent-era full body takes and Keystone Kop jumping jacks.<br /><br />Stan and Ollie do much better in a situation comedy in \\'Sons of the Desert\\'(1933) where we get to see them do what we love about them -- be themselves. In fact, 1932-34 seem to be their best years.<br /><br />Since this film does not play to any of their strengths, why bother with it? I have to give it a 3.'\n",
            "0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preview a few examples\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        #print(text_batch.numpy()[i].decode('latin1'))\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99adb30-5f2b-4dae-b5e3-a12d18ad7aab",
      "metadata": {
        "id": "b99adb30-5f2b-4dae-b5e3-a12d18ad7aab"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985d6d19-3b19-4a8e-883b-8b10521f2122",
      "metadata": {
        "id": "985d6d19-3b19-4a8e-883b-8b10521f2122"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1d8766-6c51-456e-9034-c51383680379",
      "metadata": {
        "id": "dd1d8766-6c51-456e-9034-c51383680379"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f'[{re.escape(string.punctuation)}]', ''\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4ad913-b886-40bb-9daa-d1de198d5f6d",
      "metadata": {
        "id": "fc4ad913-b886-40bb-9daa-d1de198d5f6d"
      },
      "outputs": [],
      "source": [
        "# Model constants:\n",
        "max_features = 20_000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b0d3a3-2433-4fa6-b927-cc04d4e69481",
      "metadata": {
        "id": "32b0d3a3-2433-4fa6-b927-cc04d4e69481"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad79ee2a-26e6-4fa8-9b42-691423f48ac2",
      "metadata": {
        "id": "ad79ee2a-26e6-4fa8-9b42-691423f48ac2"
      },
      "outputs": [],
      "source": [
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5945b6c1-4ca2-4b91-ab90-1d7efe595f34",
      "metadata": {
        "id": "5945b6c1-4ca2-4b91-ab90-1d7efe595f34"
      },
      "source": [
        "## Two options to vectorize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef5593c6-9bd6-4e83-86f8-5425dcd10bcb",
      "metadata": {
        "id": "ef5593c6-9bd6-4e83-86f8-5425dcd10bcb"
      },
      "source": [
        "**Option 1: Make it part of the model,** so as to obtain a model that\n",
        "preprocesses raw strings, like this:\n",
        "\n",
        "```python\n",
        "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = tf.keras.layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be07e32a-44af-4596-98d3-40bdd548ea00",
      "metadata": {
        "id": "be07e32a-44af-4596-98d3-40bdd548ea00"
      },
      "source": [
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices,\n",
        "then feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two is that option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training on\n",
        "GPU. So if you're training the model on GPU, you probably want to go with\n",
        "this option to get the best performance.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts\n",
        "raw strings as input, like in the code snippet for option 1 above. This can\n",
        "be done after training. We do this in the last section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e254ee2-f2dc-4d37-8d21-d2e4070bdfb5",
      "metadata": {
        "id": "2e254ee2-f2dc-4d37-8d21-d2e4070bdfb5"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabf27ed-1b42-4ea7-bcff-a522572657a5",
      "metadata": {
        "id": "eabf27ed-1b42-4ea7-bcff-a522572657a5"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e180cda-0d53-4265-9447-f3c8288baeba",
      "metadata": {
        "id": "1e180cda-0d53-4265-9447-f3c8288baeba"
      },
      "outputs": [],
      "source": [
        "# Do async prefetching / buffering of the data for best performance on GPU\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff72b307-3ea1-4934-a629-46b78c065123",
      "metadata": {
        "id": "ff72b307-3ea1-4934-a629-46b78c065123"
      },
      "source": [
        "## Build a model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0522c9b-9f24-47d3-8dd0-802ff9903505",
      "metadata": {
        "id": "b0522c9b-9f24-47d3-8dd0-802ff9903505"
      },
      "source": [
        "Start with a simple 1D convnet starting with an Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1832c7-027f-435e-8e50-775d5768888d",
      "metadata": {
        "id": "cb1832c7-027f-435e-8e50-775d5768888d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b93dd2-623f-46c0-ae47-4e9e184465d5",
      "metadata": {
        "id": "16b93dd2-623f-46c0-ae47-4e9e184465d5"
      },
      "outputs": [],
      "source": [
        "# A integer input for vocab indices\n",
        "inputs = tf.keras.Input(shape=(None,), dtype='int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9231dc6d-2e35-4ad9-8425-9ccf472c3e15",
      "metadata": {
        "id": "9231dc6d-2e35-4ad9-8425-9ccf472c3e15"
      },
      "outputs": [],
      "source": [
        "# Next, we add a layer to map those vocab indices into a space of\n",
        "# dimensionality 'embedding_dim'\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d1013f-993f-4a96-890a-2a336aa3d8f5",
      "metadata": {
        "id": "e0d1013f-993f-4a96-890a-2a336aa3d8f5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with Python, 3e"
      ],
      "metadata": {
        "id": "FTZ_jmFQocrv"
      },
      "id": "FTZ_jmFQocrv"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "369keZxuoarI"
      },
      "id": "369keZxuoarI",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "nxfGk24domaP",
        "outputId": "3de32f5c-d71e-4767-9efe-ad818f417005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nxfGk24domaP",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rG1iWM3woukq"
      },
      "id": "rG1iWM3woukq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:expt3]",
      "language": "python",
      "name": "conda-env-expt3-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}